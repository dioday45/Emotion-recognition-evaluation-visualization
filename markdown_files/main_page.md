## Welcome to our evaluation visualization
This is the evaluation part of a semester project realized at EPFL and named "Real-time emotion recognition in virtual reality". The goal of this project is to build a model able to recognize the 7 basic emotions, namely Happiness, Sadness, Anger, Surprise, Fear, Disgust and Neutral.

To do this, we first created a dataset of posed emotions with 26 different participants using HTC's Vive pro eye 2 along with their facial tracker.

We then trained two different machine learning models, namely Random Forest and LightGBM, on this dataset.

We are now at the evaluation stage, and for this we collected data from two participants (they were not part of the primary dataset), who were asked to express the seven emotions sequentially for a period of 25 seconds, twice. The first time with the order they wanted, and the second time with a predefined order, identical for all participants.

To see the result of our model on these data, you can change the app mode using the left panel.

Good visualization :)

